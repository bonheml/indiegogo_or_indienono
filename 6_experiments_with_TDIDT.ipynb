{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header>\n",
    "    <h1>CA4010 - Data warehousing and Data mining</h1>\n",
    "    <h2>Continuous assessment project</h2>\n",
    "</header>\n",
    "<p>\n",
    "    For this project, we want to predict if a project submitted to \n",
    "    <a href=\"https://www.indiegogo.com\">indiegogo.com</a> will or will not be funded.\n",
    "    For this purpose, we'll use a\n",
    "    <a href=\"https://www.kaggle.com/kingburrito666/indiegogo-project-statistics/data\">\n",
    "    dataset from kaggle containing one year of indiegogo projects.</a>\n",
    "    The version used here is the version cleaned in part 1, 2 and 5.\n",
    "</p>\n",
    "<p>\n",
    "    This notebook will decribe and show how we'll predict if a project will be funded or not.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification</h2>\n",
    "<p>Here we'll experiment around the classification and see if our results are correct or not. We're not expecting to have very high results as our dataset is poorly balanced with some projects having very high collected percentages, pledges count or target amount and other very low ones.</p>\n",
    "<p>As we want to predict if a project will be funded or not, we'll use a <b>classifier</b> and because our dataset contains <b>mixed attributes</b>, we'll use a <b>Decision Tree</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiegogo = pd.read_csv(\"indiegogo_cleaned_dataset.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>First, we'll have to transform our categorical datas using <b>one hot encoding</b> to have only continuous attributes in our decision tree. This encoding will be applied to:\n",
    "        <ul>\n",
    "            <li>category_slug</li>\n",
    "            <li>currency_code</li>\n",
    "        </ul>\n",
    "As <i>has_partner</i> is a boolean we don't need to apply a one hot encoding on it.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiegogo = pd.get_dummies(indiegogo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As we want to predict if a project will or will not be funded, we'll extract 2 categories from our dataset using the collected_percentage values:\n",
    "        <ul>\n",
    "            <li><b>Funded:</b> The project is successfully funded with at least a 100% score of collected percentage</li>\n",
    "            <li><b>Not funded:</b> The project is not funded (less than 100% score of collected percentage)</li>\n",
    "        </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = indiegogo.collected_percentage.map(lambda x: 'Funded' if x >= 100 else 'Not funded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = indiegogo.drop(['collected_percentage'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, we'll normalize our data to avoid domination of high scores in computation. As we know that our dataset as a very high standard deviation score for almost all its attributes, we don't want to use <b>min-max</b> normalisation as the extreme values are not representatives of our dataset. That's why we'll use <b>z-norm</b> here which is more robust for this kind of data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X.columns.values\n",
    "for col in cols:\n",
    "    X[col] = (X[col] - X[col].mean())/X[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, we'll split the dataset into train and test samples, the test sample will be 30% of the whole entries.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we'll build the decision tree using gini as splitting criterion. We don't expect to see a big difference with using entropy as the results are often very similar as explained <a href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decision-tree-binary.md\">here.</a> As our data are spread, we're not expecting a very high score for this prediction</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86659794803\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Funded       0.77      0.73      0.75     11233\n",
      " Not funded       0.90      0.92      0.91     30483\n",
      "\n",
      "avg / total       0.86      0.87      0.87     41716\n",
      "\n",
      "[[ 8151  3082]\n",
      " [ 2483 28000]]\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0, max_depth=7, criterion=\"gini\")\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(metrics.classification_report(y_test, clf.predict(X_test)))\n",
    "print(metrics.confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After few tests on max_depth, we've obtained a decision tree with a precision of <b>86.7%</b> which is way higher than expected. We can see that the precision is better on not funded projects, probably due to the fact that the number of funded projects is substentially smaller than the non funded ones. It'll be interesting to re run the experiment with a dataset containing half of both kind of projects. We can also guess that projects with extreme values for pledges count (very high pledge count and not funded or very low pledge count and funded) could be responsible of some false positives.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we'll test the decision tree with entropy as splitting criterion to see if the results are equivalents.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865974685972\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Funded       0.78      0.71      0.74     11233\n",
      " Not funded       0.90      0.92      0.91     30483\n",
      "\n",
      "avg / total       0.86      0.87      0.86     41716\n",
      "\n",
      "[[ 7947  3286]\n",
      " [ 2305 28178]]\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0, max_depth=7, criterion=\"entropy\")\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(metrics.classification_report(y_test, clf.predict(X_test)))\n",
    "print(metrics.confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As expected, we can see that we reach almost the same score <b>86.6%</b> instead of <b>86.7%</b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "<p>Our dataset was very complicated to clean as we didn't have any information about the meaning of each attribute. We had to deal with several tricky points such as:\n",
    "<ul>\n",
    "    <li>Remove duplicate projects</li>\n",
    "    <li>Find out the meaning of the balance attribute</li>\n",
    "    <li>Convert the balance to an uniform currency</li>\n",
    "    <li>Aggregate the 70 categories of projects we had using an efficient strategy</li>\n",
    "    <li>Filter out the attributes with only one value or with unique values</li>\n",
    "</ul>\n",
    "After this first cleaning, we had see that our dataset contained about 700 projects having extreme values for at least one attribute. Most of them where interesting and thus, we have chosen to keep them. Our dataset also contained mixed attributes and using one hot encoding with the 14 categories we first extracted wasn't usable. So we've had to refine the aggregation a second time more efficiently.\n",
    "We also have had to be very careful in our algorithm choice for normalisation as our dataset contained a very high standard deviation for almost all its numerical attributes and as we had mixed attributes.\n",
    "Having considered all the challenges we encontered we won't expect to get a good prediction score and were thinking to try a clustering approach to classify the projects. We were very surprised to find out that our decision tree reached a very good precision and recall.\n",
    "This project as shown us the impact of a correctly cleaned dataset and the challenges it may pose.\n",
    "It also shown us knowing our dataset is very important as we can choose the algorithms which best fit our data and that experimenting is also a key as many times the results we've found were not those we were expecting.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
